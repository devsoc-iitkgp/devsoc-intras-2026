{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a82446c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üï∏Ô∏è  Starting Universal Crawler (v3)...\n",
      "üìñ Reading Page 1...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 2...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 3...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 4...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 5...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 6...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 7...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 8...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 9...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 10...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 11...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "‚úÖ Reached end of the list (No 'Next page' link found).\n",
      "\n",
      "üéâ Crawler Finished! Found 0 total pages.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "# CONFIGURATION\n",
    "BASE_URL = \"https://wiki.metakgp.org\"\n",
    "SEED_URL = \"https://wiki.metakgp.org/w/Special:AllPages\"\n",
    "\n",
    "# Namespaces to strictly ignore\n",
    "IGNORED_NAMESPACES = [\n",
    "    \"Special:\", \"Talk:\", \"User:\", \"User_talk:\", \"Metakgp:\", \n",
    "    \"Metakgp_talk:\", \"File:\", \"File_talk:\", \"MediaWiki:\", \n",
    "    \"Template:\", \"Help:\", \"Category:\", \"Category_talk:\"\n",
    "]\n",
    "\n",
    "def crawl_all_urls():\n",
    "    print(\"üï∏Ô∏è  Starting Universal Crawler (v3)...\")\n",
    "    current_url = SEED_URL\n",
    "    all_links = []\n",
    "    page_counter = 1\n",
    "\n",
    "    while current_url:\n",
    "        print(f\"üìñ Reading Page {page_counter}...\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå Failed to load: {current_url}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # --- 1. REMOVE NOISE (Sidebar & Footer) ---\n",
    "            # We destroy the sidebar and footer from the soup object before searching.\n",
    "            # This ensures we don't accidentally grab \"Main Page\" or \"About\" links.\n",
    "            for garbage in soup.find_all(class_=['mw-panel', 'vector-menu-portal', 'footer', 'mw-footer']):\n",
    "                garbage.decompose()\n",
    "            for garbage in soup.find_all(id=['mw-panel', 'footer', 'mw-navigation']):\n",
    "                garbage.decompose()\n",
    "\n",
    "            # --- 2. FIND ALL REMAINING LINKS ---\n",
    "            # Now the only links left should be in the content area.\n",
    "            links = soup.find_all('a', href=True)\n",
    "            found_on_this_page = 0\n",
    "            \n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                full_url = urljoin(BASE_URL, href)\n",
    "                \n",
    "                # --- 3. FILTER LOGIC ---\n",
    "                # A. Must be a Wiki link\n",
    "                if \"/wiki/\" not in href:\n",
    "                    continue\n",
    "                \n",
    "                # B. Must NOT be an Admin/System page\n",
    "                is_banned = False\n",
    "                for ns in IGNORED_NAMESPACES:\n",
    "                    if ns in href:\n",
    "                        is_banned = True\n",
    "                        break\n",
    "                \n",
    "                if is_banned:\n",
    "                    continue\n",
    "\n",
    "                # C. Must NOT be the \"Next Page\" pagination link\n",
    "                if \"Next page\" in link.text or \"Previous page\" in link.text:\n",
    "                    continue\n",
    "\n",
    "                # If we passed all checks, it's a valid article!\n",
    "                all_links.append(full_url)\n",
    "                found_on_this_page += 1\n",
    "\n",
    "            print(f\"   -> Found {found_on_this_page} valid links on this page.\")\n",
    "            \n",
    "            # --- DEBUG: If 0 found, print what we DID see to help debug ---\n",
    "            if found_on_this_page == 0:\n",
    "                print(\"   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\")\n",
    "                for l in links[:5]:\n",
    "                    print(f\"      - Text: '{l.text}' | Href: '{l['href']}'\")\n",
    "\n",
    "            # --- 4. PAGINATION ---\n",
    "            # We look for the \"Next page\" link specifically.\n",
    "            next_link = None\n",
    "            # Re-fetch all links including navigation (since we decomposed them earlier, \n",
    "            # we might need to check if we deleted the nav. \n",
    "            # Actually, the 'Next' link is usually in the content body or top/bottom of list.\n",
    "            # If we decomposed 'mw-navigation', we might have killed it.\n",
    "            # Let's check the UN-MODIFIED text for pagination link.\n",
    "            \n",
    "            # Strategy: Search the raw text for the 'Next page' link pattern if soup failed\n",
    "            pagination_soup = BeautifulSoup(response.text, 'html.parser') # Fresh soup\n",
    "            nav_links = pagination_soup.find_all(\"a\", href=True)\n",
    "            \n",
    "            for link in nav_links:\n",
    "                if \"Next page\" in link.text:\n",
    "                    next_link = urljoin(BASE_URL, link['href'])\n",
    "                    break\n",
    "            \n",
    "            if next_link:\n",
    "                current_url = next_link\n",
    "                page_counter += 1\n",
    "                time.sleep(0.5)\n",
    "            else:\n",
    "                print(\"‚úÖ Reached end of the list (No 'Next page' link found).\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL ERROR: {e}\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nüéâ Crawler Finished! Found {len(all_links)} total pages.\")\n",
    "    return all_links\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_list = crawl_all_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "390d3553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\Gaurav Gupta\\AppData\\Local\\Temp\\ipykernel_14740\\4238407172.py:8: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  INPUT_DIR = \"C:\\programming\\prg\\Devsoc-hackathon\\scraped_data\"  # Directory containing your batch_*.json files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading data from 24 files...\n",
      "‚úÖ Loaded 3582 source pages.\n",
      "üï∏Ô∏è  Generating Graph Chunks...\n",
      "‚úÖ Generated 9344 Graph Chunks.\n",
      "\n",
      "--- SAMPLE GRAPH CHUNK ---\n",
      "\n",
      "SOURCE_PAGE: AG60044: Advanced Groundwater Hydrology\n",
      "LAST_UPDATED: Unknown\n",
      "RELATED_TOPICS: \n",
      "---------------------\n",
      "meteorological fluctuations, tidal fluctuations, impacts of urbanization,\n",
      "earthquakes and external loads on groundwater, land subsidence.\n",
      "Aquifer Tests and Parameter Estimation: Need of aquifer tests, type and design\n",
      "of aquifer tests, test procedures, merits and demerits of pumping test, steady\n",
      "and transient methods for determining aquifer parameters from pumping test\n",
      "data, recovery test, analysis of step-drawdown test data, overview of slug\n",
      "tests.\n",
      "Groundwater Quality and Contamination: Definitions, water-quality parameters\n",
      "and characteristics, monitoring of groundwater quality, water-quality criteria\n",
      "and standards, collection of groundwater samples, vadose zone monitoring,\n",
      "groundwater contamination, sources and causes of groundwater contamination,\n",
      "\n",
      "\n",
      "--- METADATA ---\n",
      "{'source': 'https://wiki.metakgp.org/w/AG60044:_Advanced_Groundwater_Hydrology', 'title': 'AG60044: Advanced Groundwater Hydrology', 'chunk_id': 1, 'last_modified': 'Unknown', 'graph_neighbors': []}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# CONFIGURATION\n",
    "INPUT_DIR = \"C:\\programming\\prg\\Devsoc-hackathon\\scraped_data\"  # Directory containing your batch_*.json files\n",
    "OUTPUT_FILE = \"graph_chunks.json\" # Where we save the processed chunks (optional debug)\n",
    "\n",
    "def load_all_data(directory):\n",
    "    \"\"\"Loads all batch JSON files into a single list.\"\"\"\n",
    "    all_pages = []\n",
    "    files = glob.glob(os.path.join(directory, \"batch_*.json\"))\n",
    "    print(f\"üìÇ Loading data from {len(files)} files...\")\n",
    "    \n",
    "    for f_path in files:\n",
    "        try:\n",
    "            with open(f_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                all_pages.extend(data)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error reading {f_path}: {e}\")\n",
    "            \n",
    "    print(f\"‚úÖ Loaded {len(all_pages)} source pages.\")\n",
    "    return all_pages\n",
    "\n",
    "def create_graph_chunks():\n",
    "    # 1. Load Data\n",
    "    raw_pages = load_all_data(INPUT_DIR)\n",
    "    \n",
    "    # 2. Define the Splitter\n",
    "    # We use a smaller chunk size to keep facts precise.\n",
    "    # Overlap is critical to not cut a sentence in half.\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    graph_documents = []\n",
    "    \n",
    "    print(\"üï∏Ô∏è  Generating Graph Chunks...\")\n",
    "    \n",
    "    for page in raw_pages:\n",
    "        # Extract Core Data\n",
    "        title = page.get('title', 'Unknown')\n",
    "        url = page.get('url', 'Unknown')\n",
    "        last_mod = page.get('last_modified', 'Unknown')\n",
    "        \n",
    "        # --- GRAPH EDGE LOGIC ---\n",
    "        # The 'graph_connections' list (from your scraper) is the KEY.\n",
    "        # We must attach these neighbors to *every* chunk of this page.\n",
    "        neighbors = page.get('graph_connections', [])\n",
    "        neighbors_str = \", \".join(neighbors[:50]) # Limit to 50 links to save space\n",
    "        \n",
    "        # Clean Content\n",
    "        content = page.get('content', '')\n",
    "        if not content: continue\n",
    "            \n",
    "        # Split the content\n",
    "        text_chunks = splitter.split_text(content)\n",
    "        \n",
    "        for i, chunk_text in enumerate(text_chunks):\n",
    "            \n",
    "            # --- THE \"GRAPH CHUNK\" MAGIC ---\n",
    "            # We inject the metadata directly into the TEXT so the LLM reads it.\n",
    "            # This allows the LLM to say: \"I see a link to 'Gymkhana' here, let me ask about that.\"\n",
    "            \n",
    "            contextualized_text = f\"\"\"\n",
    "SOURCE_PAGE: {title}\n",
    "LAST_UPDATED: {last_mod}\n",
    "RELATED_TOPICS: {neighbors_str}\n",
    "---------------------\n",
    "{chunk_text}\n",
    "\"\"\"\n",
    "            # Create the Document Object (Standard LangChain format)\n",
    "            # We also keep clean metadata for code-level filtering\n",
    "            doc = Document(\n",
    "                page_content=contextualized_text,\n",
    "                metadata={\n",
    "                    \"source\": url,\n",
    "                    \"title\": title,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"last_modified\": last_mod,\n",
    "                    \"graph_neighbors\": neighbors # Keep the raw list for code logic\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            graph_documents.append(doc)\n",
    "\n",
    "    print(f\"‚úÖ Generated {len(graph_documents)} Graph Chunks.\")\n",
    "    return graph_documents\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    final_chunks = create_graph_chunks()\n",
    "    \n",
    "    # Debug: Print one chunk to see the structure\n",
    "    if final_chunks:\n",
    "        print(\"\\n--- SAMPLE GRAPH CHUNK ---\")\n",
    "        print(final_chunks[20].page_content)\n",
    "        print(\"\\n--- METADATA ---\")\n",
    "        print(final_chunks[20].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93829d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Loading Model: sentence-transformers/all-mpnet-base-v2 on GPU...\n",
      "üöÄ Starting Knowledge Graph Ingestion...\n",
      "üìÇ Loading data from 24 files...\n",
      "‚úÖ Loaded 3582 source pages.\n",
      "üï∏Ô∏è  Generating Graph Chunks...\n",
      "‚úÖ Generated 9344 Graph Chunks.\n",
      "üß© Prepared 9344 Graph-Enhanced Chunks.\n",
      "üîß Sanitizing metadata for ChromaDB compatibility...\n",
      "üß† Loading Model: sentence-transformers/all-mpnet-base-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1a5302f7-4a45-48cb-8ce5-6f1924bb38df)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/./config_sentence_transformers.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving to C:/programming/prg/Devsoc-hackathon/chroma_db_graph...\n",
      "   -> Indexing Batch 1/94\n",
      "   -> Indexing Batch 2/94\n",
      "   -> Indexing Batch 3/94\n",
      "   -> Indexing Batch 4/94\n",
      "   -> Indexing Batch 5/94\n",
      "   -> Indexing Batch 6/94\n",
      "   -> Indexing Batch 7/94\n",
      "   -> Indexing Batch 8/94\n",
      "   -> Indexing Batch 9/94\n",
      "   -> Indexing Batch 10/94\n",
      "   -> Indexing Batch 11/94\n",
      "   -> Indexing Batch 12/94\n",
      "   -> Indexing Batch 13/94\n",
      "   -> Indexing Batch 14/94\n",
      "   -> Indexing Batch 15/94\n",
      "   -> Indexing Batch 16/94\n",
      "   -> Indexing Batch 17/94\n",
      "   -> Indexing Batch 18/94\n",
      "   -> Indexing Batch 19/94\n",
      "   -> Indexing Batch 20/94\n",
      "   -> Indexing Batch 21/94\n",
      "   -> Indexing Batch 22/94\n",
      "   -> Indexing Batch 23/94\n",
      "   -> Indexing Batch 24/94\n",
      "   -> Indexing Batch 25/94\n",
      "   -> Indexing Batch 26/94\n",
      "   -> Indexing Batch 27/94\n",
      "   -> Indexing Batch 28/94\n",
      "   -> Indexing Batch 29/94\n",
      "   -> Indexing Batch 30/94\n",
      "   -> Indexing Batch 31/94\n",
      "   -> Indexing Batch 32/94\n",
      "   -> Indexing Batch 33/94\n",
      "   -> Indexing Batch 34/94\n",
      "   -> Indexing Batch 35/94\n",
      "   -> Indexing Batch 36/94\n",
      "   -> Indexing Batch 37/94\n",
      "   -> Indexing Batch 38/94\n",
      "   -> Indexing Batch 39/94\n",
      "   -> Indexing Batch 40/94\n",
      "   -> Indexing Batch 41/94\n",
      "   -> Indexing Batch 42/94\n",
      "   -> Indexing Batch 43/94\n",
      "   -> Indexing Batch 44/94\n",
      "   -> Indexing Batch 45/94\n",
      "   -> Indexing Batch 46/94\n",
      "   -> Indexing Batch 47/94\n",
      "   -> Indexing Batch 48/94\n",
      "   -> Indexing Batch 49/94\n",
      "   -> Indexing Batch 50/94\n",
      "   -> Indexing Batch 51/94\n",
      "   -> Indexing Batch 52/94\n",
      "   -> Indexing Batch 53/94\n",
      "   -> Indexing Batch 54/94\n",
      "   -> Indexing Batch 55/94\n",
      "   -> Indexing Batch 56/94\n",
      "   -> Indexing Batch 57/94\n",
      "   -> Indexing Batch 58/94\n",
      "   -> Indexing Batch 59/94\n",
      "   -> Indexing Batch 60/94\n",
      "   -> Indexing Batch 61/94\n",
      "   -> Indexing Batch 62/94\n",
      "   -> Indexing Batch 63/94\n",
      "   -> Indexing Batch 64/94\n",
      "   -> Indexing Batch 65/94\n",
      "   -> Indexing Batch 66/94\n",
      "   -> Indexing Batch 67/94\n",
      "   -> Indexing Batch 68/94\n",
      "   -> Indexing Batch 69/94\n",
      "   -> Indexing Batch 70/94\n",
      "   -> Indexing Batch 71/94\n",
      "   -> Indexing Batch 72/94\n",
      "   -> Indexing Batch 73/94\n",
      "   -> Indexing Batch 74/94\n",
      "   -> Indexing Batch 75/94\n",
      "   -> Indexing Batch 76/94\n",
      "   -> Indexing Batch 77/94\n",
      "   -> Indexing Batch 78/94\n",
      "   -> Indexing Batch 79/94\n",
      "   -> Indexing Batch 80/94\n",
      "   -> Indexing Batch 81/94\n",
      "   -> Indexing Batch 82/94\n",
      "   -> Indexing Batch 83/94\n",
      "   -> Indexing Batch 84/94\n",
      "   -> Indexing Batch 85/94\n",
      "   -> Indexing Batch 86/94\n",
      "   -> Indexing Batch 87/94\n",
      "   -> Indexing Batch 88/94\n",
      "   -> Indexing Batch 89/94\n",
      "   -> Indexing Batch 90/94\n",
      "   -> Indexing Batch 91/94\n",
      "   -> Indexing Batch 92/94\n",
      "   -> Indexing Batch 93/94\n",
      "   -> Indexing Batch 94/94\n",
      "‚úÖ Knowledge Graph Successfully Built!\n",
      "   You can now query this DB at: C:/programming/prg/Devsoc-hackathon/chroma_db_graph\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# CONFIGURATION\n",
    "DB_DIR = \"C:/programming/prg/Devsoc-hackathon/chroma_db_graph\"  # Separate DB for Graph RAG\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# 2. Force GPU Usage (Crucial for Speed!)\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # MPNet performs better with normalization\n",
    "\n",
    "print(f\"üß† Loading Model: {EMBEDDING_MODEL} on GPU...\")\n",
    "\n",
    "def ingest_knowledge_graph():\n",
    "    # 1. Generate the \"Smart\" Graph Chunks\n",
    "    print(\"üöÄ Starting Knowledge Graph Ingestion...\")\n",
    "    graph_docs = create_graph_chunks()\n",
    "    \n",
    "    if not graph_docs:\n",
    "        print(\"‚ùå No documents found. Check your json files.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üß© Prepared {len(graph_docs)} Graph-Enhanced Chunks.\")\n",
    "\n",
    "    print(\"üîß Sanitizing metadata for ChromaDB compatibility...\")\n",
    "    for doc in graph_docs:\n",
    "        if \"graph_neighbors\" in doc.metadata:\n",
    "            # Convert ['Link A', 'Link B'] -> \"Link A, Link B\"\n",
    "            neighbors = doc.metadata[\"graph_neighbors\"]\n",
    "            if isinstance(neighbors, list):\n",
    "                doc.metadata[\"graph_neighbors\"] = \", \".join(neighbors)\n",
    "            else:\n",
    "                doc.metadata[\"graph_neighbors\"] = str(neighbors)\n",
    "\n",
    "    # 2. Initialize the Embedding Model (The \"Translator\" to Math)\n",
    "    print(f\"üß† Loading Model: {EMBEDDING_MODEL}...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "    # 3. Store in Vector Database (The \"Memory\")\n",
    "    print(f\"üíæ Saving to {DB_DIR}...\")\n",
    "    \n",
    "    # We use batching to ensure we don't crash memory\n",
    "    BATCH_SIZE = 100\n",
    "    total_batches = (len(graph_docs) // BATCH_SIZE) + 1\n",
    "    \n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=DB_DIR, \n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    \n",
    "    for i in range(0, len(graph_docs), BATCH_SIZE):\n",
    "        batch = graph_docs[i : i + BATCH_SIZE]\n",
    "        print(f\"   -> Indexing Batch {i//BATCH_SIZE + 1}/{total_batches}\")\n",
    "        vectorstore.add_documents(batch)\n",
    "        \n",
    "    print(\"‚úÖ Knowledge Graph Successfully Built!\")\n",
    "    print(f\"   You can now query this DB at: {DB_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ingest_knowledge_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5efb7af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Loading Model...\n",
      "\n",
      "üîé Query: Who are the governors of the Technology Literary Society?\n",
      "\n",
      "--- Result 1 ---\n",
      "üìÑ Source: Technology Literary Society\n",
      "üîó Related: ...\n",
      "üìù Text Snippet:\n",
      "\n",
      "SOURCE_PAGE: Technology Literary Society\n",
      "LAST_UPDATED: Unknown\n",
      "RELATED_TOPICS: \n",
      "---------------------\n",
      "From the year 2010-11, there have been only 4 Governors in TLS every year.\n",
      "There has also been a ...\n",
      "\n",
      "--- Result 2 ---\n",
      "üìÑ Source: Technology Literary Society\n",
      "üîó Related: ...\n",
      "üìù Text Snippet:\n",
      "\n",
      "SOURCE_PAGE: Technology Literary Society\n",
      "LAST_UPDATED: Unknown\n",
      "RELATED_TOPICS: \n",
      "---------------------\n",
      "From the year 2010-11, there have been only 4 Governors in TLS every year.\n",
      "There has also been a ...\n",
      "\n",
      "--- Result 3 ---\n",
      "üìÑ Source: Constitution of the Technology Students' Gymkhana\n",
      "üîó Related: ...\n",
      "üìù Text Snippet:\n",
      "\n",
      "SOURCE_PAGE: Constitution of the Technology Students' Gymkhana\n",
      "LAST_UPDATED: Unknown\n",
      "RELATED_TOPICS: \n",
      "---------------------\n",
      "a) Entertainment Subcommittee\n",
      "b) Dramatics Subcommittee\n",
      "c) Journal Subcommi...\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# CONFIGURATION\n",
    "DB_DIR = \"C:/programming/prg/Devsoc-hackathon/chroma_db_graph\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# 1. Initialize\n",
    "print(\"üß† Loading Model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs={'device': 'cuda'}, # Use your RTX 4050\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "vectorstore = Chroma(persist_directory=DB_DIR, embedding_function=embeddings)\n",
    "\n",
    "# 2. Ask a Question\n",
    "query = \"Who are the governors of the Technology Literary Society?\"\n",
    "print(f\"\\nüîé Query: {query}\")\n",
    "\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "# 3. Show Results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(f\"üìÑ Source: {doc.metadata['title']}\")\n",
    "    print(f\"üîó Related: {doc.metadata.get('graph_neighbors', 'None')[:50]}...\")\n",
    "    print(f\"üìù Text Snippet:\\n{doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d91d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Initializing Graph Agent (Groq Llama-3.3-70B)...\n",
      "\n",
      "üöÄ STARTING TRAVERSAL: 'Who is Elon Musk'\n",
      "\n",
      "üë£ Step 1: Searching for 'Who is Elon Musk'...\n",
      "   üìÑ Found Node: EP60007: Techno-Entrepreneurial Leadership\n",
      "   ü§î Decision: HOP: Techno-Entrepreneurial Leadership\n",
      "   üîó Hopping to -> Techno-Entrepreneurial Leadership\n",
      "\n",
      "üë£ Step 2: Searching for 'Techno-Entrepreneurial Leadership'...\n",
      "   üìÑ Found Node: EP60007: Techno-Entrepreneurial Leadership\n",
      "   ü§î Decision: HOP: Elon Musk\n",
      "   üîó Hopping to -> Elon Musk\n",
      "\n",
      "üë£ Step 3: Searching for 'Elon Musk'...\n",
      "   üìÑ Found Node: EP60007: Techno-Entrepreneurial Leadership\n",
      "   ü§î Decision: HOP: Elon Musk\n",
      "   üîó Hopping to -> Elon Musk\n",
      "\n",
      "========================================\n",
      "FINAL RESULT:\n",
      "========================================\n",
      "‚ùå Max steps reached without a final answer.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# --- CHANGE 1: Import Groq ---\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "DB_DIR = \"C:/programming/prg/Devsoc-hackathon/chroma_db_graph\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# --- CHANGE 2: Paste your Groq Key here ---\n",
    "# Get it from: https://console.groq.com/keys\n",
    "MY_GROQ_KEY = \"\"\n",
    "\n",
    "class GraphRAGAgent:\n",
    "    def __init__(self):\n",
    "        print(\"üß† Initializing Graph Agent (Groq Llama-3.3-70B)...\")\n",
    "        \n",
    "        # 1. Load Memory (Local GPU)\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL,\n",
    "            model_kwargs={'device': 'cuda'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        self.db = Chroma(persist_directory=DB_DIR, embedding_function=self.embeddings)\n",
    "        \n",
    "        # 2. Load Brain (Groq)\n",
    "        if not MY_GROQ_KEY or \"...\" in MY_GROQ_KEY:\n",
    "            raise ValueError(\"‚ùå You forgot to paste your Groq API Key!\")\n",
    "\n",
    "        # --- CHANGE 3: Initialize ChatGroq ---\n",
    "        self.llm = ChatGroq(\n",
    "            api_key=MY_GROQ_KEY,\n",
    "            model=\"llama-3.3-70b-versatile\", # Powerful & Fast model\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # 3. Navigator Prompt (Kept exactly the same)\n",
    "        self.navigator_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are a precise Graph RAG Agent.\n",
    "        \n",
    "        GOAL: {goal}\n",
    "        \n",
    "        CONTEXT FROM DATABASE:\n",
    "        --------------------------------------------------\n",
    "        {context}\n",
    "        --------------------------------------------------\n",
    "        \n",
    "        INSTRUCTIONS:\n",
    "        1. Analyze the context and the \"RELATED_TOPICS\" links.\n",
    "        2. If the text answers the GOAL, reply ONLY with:\n",
    "           ANSWER: [The answer]\n",
    "        3. If you need to search a related topic to find the answer, reply ONLY with:\n",
    "           HOP: [Topic Name]\n",
    "           \n",
    "        CONSTRAINT: Output ONLY the line starting with ANSWER or HOP. No explanations.\n",
    "        \"\"\")\n",
    "        \n",
    "    def search(self, query):\n",
    "        results = self.db.similarity_search(query, k=1)\n",
    "        return results[0] if results else None\n",
    "\n",
    "    def solve(self, user_query, max_hops=3):\n",
    "        print(f\"\\nüöÄ STARTING TRAVERSAL: '{user_query}'\")\n",
    "        current_query = user_query\n",
    "        visited_context = []\n",
    "        \n",
    "        for step in range(max_hops):\n",
    "            print(f\"\\nüë£ Step {step + 1}: Searching for '{current_query}'...\")\n",
    "            \n",
    "            # Keep sleep to avoid rate limits (Groq has limits too!)\n",
    "            time.sleep(2) \n",
    "            \n",
    "            node = self.search(current_query)\n",
    "            if not node:\n",
    "                print(\"   ‚ùå Dead end. No information found.\")\n",
    "                break\n",
    "                \n",
    "            content = node.page_content\n",
    "            source = node.metadata.get('title', 'Unknown')\n",
    "            neighbors = node.metadata.get('graph_neighbors', 'None')\n",
    "            \n",
    "            print(f\"   üìÑ Found Node: {source}\")\n",
    "            \n",
    "            visited_context.append(f\"SOURCE: {source}\\nRELATED_TOPICS: {neighbors}\\nCONTENT: {content}\")\n",
    "            full_context = \"\\n\\n\".join(visited_context)\n",
    "            \n",
    "            chain = self.navigator_prompt | self.llm | StrOutputParser()\n",
    "            try:\n",
    "                decision = chain.invoke({\"goal\": user_query, \"context\": full_context})\n",
    "                decision = decision.strip()\n",
    "                print(f\"   ü§î Decision: {decision}\")\n",
    "                \n",
    "                match = re.search(r\"(ANSWER|HOP):\\s*(.*)\", decision, re.DOTALL)\n",
    "                \n",
    "                if match:\n",
    "                    action = match.group(1)\n",
    "                    value = match.group(2).strip()\n",
    "                    \n",
    "                    # ... inside solve() method, when you find an ANSWER ...\n",
    "\n",
    "                    if action == \"ANSWER\":\n",
    "                        # --- MOE INTEGRATION ---\n",
    "                        print(\"\\n‚úã Holding Answer for Verification...\")\n",
    "                        \n",
    "                        # We pass the full history of text we read as \"Context\"\n",
    "                        full_context_log = \"\\n\".join(visited_context)\n",
    "                        \n",
    "                        is_valid, reason = self.verifier.verify(user_query, value, full_context_log)\n",
    "                        \n",
    "                        if is_valid:\n",
    "                            return value\n",
    "                        else:\n",
    "                            return f\"‚ùå Verification Failed: {reason} \\n(Original Draft: {value})\"\n",
    "                    elif action == \"HOP\":\n",
    "                        print(f\"   üîó Hopping to -> {value}\")\n",
    "                        current_query = value\n",
    "                else:\n",
    "                    if step == max_hops - 1:\n",
    "                        return decision\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return f\"‚ùå Error: {e}\"\n",
    "\n",
    "        return \"‚ùå Max steps reached without a final answer.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = GraphRAGAgent()\n",
    "    \n",
    "    # Test Query\n",
    "    answer = agent.solve(\"Who is Elon Musk\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"FINAL RESULT:\")\n",
    "    print(\"=\"*40)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6bdba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Initializing Graph Agent & Verifiers (Gemini Flash)...\n",
      "Asking: What is Gymkhana?\n",
      "\n",
      "üöÄ THINKING PROCESS: 'What is Gymkhana?'\n",
      "   üë£ Step 1: Searching for 'What is Gymkhana?'...\n",
      "      üìÑ Reading: Constitution of the Technology Students' Gymkhana\n",
      "      üîó Hopping to -> Gymkhana definition\n",
      "   üë£ Step 2: Searching for 'Gymkhana definition'...\n",
      "      üìÑ Reading: Constitution of the Technology Students' Gymkhana\n",
      "      üîó Hopping to -> Gymkhana (general term)\n",
      "   üë£ Step 3: Searching for 'Gymkhana (general term)'...\n",
      "      üìÑ Reading: Sports Facilities\n",
      "\n",
      "   üïµÔ∏è  MoE Verifier is grading the answer...\n",
      "      ‚úÖ Verified.\n",
      "FINAL ANSWER: Gymkhana, specifically the Technology Students' Gymkhana, is a student-managed hub for numerous extra-curricular and co-curricular activities, ranging from sports to music. It provides various facilities such as a gym, squash courts, billiards, and a swimming pool, which may require fees and forms available on its website.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# --- CHANGE 1: Import Google Gemini ---\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "DB_DIR = \"C:/programming/prg/Devsoc-hackathon/chroma_db_graph\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# --- CHANGE 2: Paste your Google Gemini Key here ---\n",
    "# Get it from: https://aistudio.google.com/app/apikey\n",
    "MY_GEMINI_KEY = \"\"\n",
    "\n",
    "# ==========================================\n",
    "# 1. THE MIXTURE OF EXPERTS (THE AUDITOR)\n",
    "# ==========================================\n",
    "class MoEVerifier:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def verify(self, question, answer, context_used):\n",
    "        print(f\"\\n   üïµÔ∏è  MoE Verifier is grading the answer...\")\n",
    "        \n",
    "        # --- EXPERT 1: SOURCE MATCHER ---\n",
    "        source_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are a strict Fact Checker.\n",
    "        \n",
    "        CONTEXT FROM DATABASE:\n",
    "        {context}\n",
    "        \n",
    "        PROPOSED ANSWER:\n",
    "        {answer}\n",
    "        \n",
    "        TASK:\n",
    "        Does the CONTEXT fully support the PROPOSED ANSWER? \n",
    "        If the answer contains names, dates, or facts NOT in the context, you must flag it.\n",
    "        \n",
    "        OUTPUT JSON: {{ \"is_supported\": boolean, \"reason\": \"string\" }}\n",
    "        \"\"\")\n",
    "        \n",
    "        try:\n",
    "            chain1 = source_prompt | self.llm | JsonOutputParser()\n",
    "            result1 = chain1.invoke({\"context\": context_used, \"answer\": answer})\n",
    "            \n",
    "            if not result1['is_supported']:\n",
    "                print(f\"      ‚ùå REJECTED by Source Matcher: {result1['reason']}\")\n",
    "                return False, f\"Hallucination Detected: {result1['reason']}\"\n",
    "        except:\n",
    "            pass # If json parse fails, we skip strict check to be safe\n",
    "\n",
    "        # --- EXPERT 2: LOGIC GUARD ---\n",
    "        logic_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are a Logic Analyst.\n",
    "        \n",
    "        QUESTION: {question}\n",
    "        ANSWER: {answer}\n",
    "        \n",
    "        TASK:\n",
    "        Does the ANSWER directly address the QUESTION?\n",
    "        \n",
    "        OUTPUT JSON: {{ \"is_relevant\": boolean, \"reason\": \"string\" }}\n",
    "        \"\"\")\n",
    "        \n",
    "        try:\n",
    "            chain2 = logic_prompt | self.llm | JsonOutputParser()\n",
    "            result2 = chain2.invoke({\"question\": question, \"answer\": answer})\n",
    "            \n",
    "            if not result2['is_relevant']:\n",
    "                print(f\"      ‚ùå REJECTED by Logic Guard: {result2['reason']}\")\n",
    "                return False, f\"Irrelevant Answer: {result2['reason']}\"\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(\"      ‚úÖ Verified.\")\n",
    "        return True, \"Verified\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. THE GRAPH AGENT (THE RESEARCHER)\n",
    "# ==========================================\n",
    "class GraphRAGAgent:\n",
    "    def __init__(self):\n",
    "        print(\"üß† Initializing Graph Agent & Verifiers (Gemini Flash)...\")\n",
    "        \n",
    "        # Load Memory\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL,\n",
    "            model_kwargs={'device': 'cuda'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        self.db = Chroma(persist_directory=DB_DIR, embedding_function=self.embeddings)\n",
    "        \n",
    "        # Load Brain\n",
    "        if not MY_GEMINI_KEY or \"...\" in MY_GEMINI_KEY:\n",
    "            raise ValueError(\"‚ùå You forgot to paste your Gemini API Key!\")\n",
    "\n",
    "        # --- CHANGE 3: Initialize Gemini ---\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            google_api_key=MY_GEMINI_KEY,\n",
    "            model=\"gemini-2.5-flash\", # Or \"gemini-2.5-flash-exp\" if you have access\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Attach Verifier\n",
    "        self.verifier = MoEVerifier(self.llm)\n",
    "        \n",
    "        # Navigator Prompt\n",
    "        self.navigator_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are a Research Agent.\n",
    "        \n",
    "        GOAL: {goal}\n",
    "        \n",
    "        CURRENT INFORMATION FOUND:\n",
    "        {context}\n",
    "        \n",
    "        INSTRUCTIONS:\n",
    "        1. Read the text. \n",
    "        2. If you have the COMPLETE answer, output: ANSWER: [Your Answer]\n",
    "        3. If you need to search a related topic, output: HOP: [Topic Name]\n",
    "        \n",
    "        CONSTRAINT: Output ONLY the line starting with ANSWER or HOP.\n",
    "        \"\"\")\n",
    "        \n",
    "    def search(self, query):\n",
    "        results = self.db.similarity_search(query, k=1)\n",
    "        return results[0] if results else None\n",
    "\n",
    "    def solve(self, user_query, max_hops=3):\n",
    "        print(f\"\\nüöÄ THINKING PROCESS: '{user_query}'\")\n",
    "        current_query = user_query\n",
    "        visited_context = []\n",
    "        \n",
    "        for step in range(max_hops):\n",
    "            print(f\"   üë£ Step {step + 1}: Searching for '{current_query}'...\")\n",
    "            \n",
    "            # Gemini has higher rate limits, but a small sleep is still good practice\n",
    "            time.sleep(1) \n",
    "            \n",
    "            node = self.search(current_query)\n",
    "            if not node:\n",
    "                print(\"      ‚ö†Ô∏è Dead end in graph.\")\n",
    "                break\n",
    "                \n",
    "            content = node.page_content\n",
    "            source = node.metadata.get('title', 'Unknown')\n",
    "            neighbors = node.metadata.get('graph_neighbors', 'None')\n",
    "            \n",
    "            print(f\"      üìÑ Reading: {source}\")\n",
    "            \n",
    "            # Store context for the Verifier to check later\n",
    "            visited_context.append(f\"SOURCE: {source}\\nCONTENT: {content}\\nLINKS: {neighbors}\")\n",
    "            full_context = \"\\n\\n\".join(visited_context)\n",
    "            \n",
    "            # Ask LLM what to do\n",
    "            chain = self.navigator_prompt | self.llm | StrOutputParser()\n",
    "            try:\n",
    "                decision = chain.invoke({\"goal\": user_query, \"context\": full_context})\n",
    "                decision = decision.strip()\n",
    "                \n",
    "                # Parse Decision\n",
    "                match = re.search(r\"(ANSWER|HOP):\\s*(.*)\", decision, re.DOTALL)\n",
    "                if match:\n",
    "                    action = match.group(1)\n",
    "                    value = match.group(2).strip()\n",
    "                    \n",
    "                    if action == \"ANSWER\":\n",
    "                        # *** CRITICAL STEP: VERIFY BEFORE RETURNING ***\n",
    "                        is_valid, reason = self.verifier.verify(user_query, value, full_context)\n",
    "                        \n",
    "                        if is_valid:\n",
    "                            return value\n",
    "                        else:\n",
    "                            return f\"‚ùå I found an answer, but my internal auditor rejected it.\\nReason: {reason}\"\n",
    "                    \n",
    "                    elif action == \"HOP\":\n",
    "                        print(f\"      üîó Hopping to -> {value}\")\n",
    "                        current_query = value\n",
    "                else:\n",
    "                    if step == max_hops - 1: return decision\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return f\"‚ùå Error: {e}\"\n",
    "\n",
    "        return \"‚ùå I searched the graph but could not find a complete answer within the limit.\"\n",
    "\n",
    "# ==========================================\n",
    "# 3. THE CHAT LOOP (USER INTERFACE)\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    agent = GraphRAGAgent()\n",
    "    \n",
    "    # PUT YOUR QUESTION HERE\n",
    "    question = \"What is Gymkhana?\"\n",
    "    \n",
    "    print(f\"Asking: {question}\")\n",
    "    response = agent.solve(question)\n",
    "    \n",
    "    print(\"FINAL ANSWER:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Initializing Graph Agent & MoE Verifiers (Gemini Flash)...\n",
      "Asking: what is civil department?\n",
      "\n",
      "üöÄ THINKING PROCESS: 'what is civil department?'\n",
      "   üë£ Step 1: Searching for 'what is civil department?'...\n",
      "      üìÑ Reading: Civil Services Club\n",
      "FINAL ANSWER: ‚ùå Error: Error calling model 'gemini-2.0-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 37.815680679s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '37s'}]}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "DB_DIR = \"C:/programming/prg/Devsoc-hackathon/chroma_db_graph\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "MY_GEMINI_KEY = \"\"\n",
    "\n",
    "# ==========================================\n",
    "# 1. THE MIXTURE OF EXPERTS (THE AUDITOR)\n",
    "# ==========================================\n",
    "class MoEVerifier:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def verify(self, question, answer, context_used):\n",
    "        print(f\"\\n   üïµÔ∏è  MoE Verification Council is in session...\")\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # EXPERT 1: SOURCE MATCHER\n",
    "        # Role: Checks if the answer is physically present in the source text.\n",
    "        # ---------------------------------------------------------\n",
    "        print(\"      üîç Expert 1 (Source Matcher) checking evidence...\")\n",
    "        source_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are the Source Matcher Expert.\n",
    "        \n",
    "        SOURCE TEXT:\n",
    "        {context}\n",
    "        \n",
    "        CLAIM:\n",
    "        {answer}\n",
    "        \n",
    "        TASK:\n",
    "        Does the SOURCE TEXT contain the specific information required to support the CLAIM?\n",
    "        Ignore logic or reasoning. Just check if the data points (names, dates, numbers) exist in the source.\n",
    "        \n",
    "        OUTPUT JSON: {{ \"is_supported\": boolean, \"reason\": \"string\" }}\n",
    "        \"\"\")\n",
    "        \n",
    "        try:\n",
    "            chain1 = source_prompt | self.llm | JsonOutputParser()\n",
    "            result1 = chain1.invoke({\"context\": context_used, \"answer\": answer})\n",
    "            if not result1['is_supported']:\n",
    "                print(f\"      ‚ùå REJECTED by Source Matcher: {result1['reason']}\")\n",
    "                return False, f\"Source Error: {result1['reason']}\"\n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ö†Ô∏è Expert 1 Error: {e}\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # EXPERT 2: HALLUCINATION HUNTER\n",
    "        # Role: Checks for invented details or external knowledge usage.\n",
    "        # ---------------------------------------------------------\n",
    "        print(\"      üëª Expert 2 (Hallucination Hunter) scanning for inventions...\")\n",
    "        hallucination_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are the Hallucination Hunter.\n",
    "        \n",
    "        SOURCE TEXT:\n",
    "        {context}\n",
    "        \n",
    "        GENERATED ANSWER:\n",
    "        {answer}\n",
    "        \n",
    "        TASK:\n",
    "        Did the answer invent any details that are NOT in the source text?\n",
    "        Even if the fact is true in the real world (like \"The sun is hot\"), if it is NOT in the source text, it is a HALLUCINATION.\n",
    "        \n",
    "        OUTPUT JSON: {{ \"is_clean\": boolean, \"reason\": \"string\" }}\n",
    "        \"\"\")\n",
    "        \n",
    "        try:\n",
    "            chain2 = hallucination_prompt | self.llm | JsonOutputParser()\n",
    "            result2 = chain2.invoke({\"context\": context_used, \"answer\": answer})\n",
    "            if not result2['is_clean']:\n",
    "                print(f\"      ‚ùå REJECTED by Hallucination Hunter: {result2['reason']}\")\n",
    "                return False, f\"Hallucination Detected: {result2['reason']}\"\n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ö†Ô∏è Expert 2 Error: {e}\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # EXPERT 3: LOGIC EXPERT\n",
    "        # Role: Checks if the conclusion actually follows from the premises.\n",
    "        # ---------------------------------------------------------\n",
    "        print(\"      üß† Expert 3 (Logic Expert) validating reasoning...\")\n",
    "        logic_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are the Logic Expert.\n",
    "        \n",
    "        USER QUESTION: {question}\n",
    "        DERIVED ANSWER: {answer}\n",
    "        PREMISES (CONTEXT): {context}\n",
    "        \n",
    "        TASK:\n",
    "        Does the conclusion (Answer) logically follow from the Premises?\n",
    "        Check for logical fallacies, jumping to conclusions, or answering a different question than asked.\n",
    "        \n",
    "        OUTPUT JSON: {{ \"is_logical\": boolean, \"reason\": \"string\" }}\n",
    "        \"\"\")\n",
    "        \n",
    "        try:\n",
    "            chain3 = logic_prompt | self.llm | JsonOutputParser()\n",
    "            result3 = chain3.invoke({\"question\": question, \"answer\": answer, \"context\": context_used})\n",
    "            if not result3['is_logical']:\n",
    "                print(f\"      ‚ùå REJECTED by Logic Expert: {result3['reason']}\")\n",
    "                return False, f\"Logic Error: {result3['reason']}\"\n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ö†Ô∏è Expert 3 Error: {e}\")\n",
    "\n",
    "        print(\"      ‚úÖ All Experts Passed. Answer Verified.\")\n",
    "        return True, \"Verified\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. THE GRAPH AGENT (THE RESEARCHER)\n",
    "# ==========================================\n",
    "class GraphRAGAgent:\n",
    "    def __init__(self):\n",
    "        print(\"üß† Initializing Graph Agent & MoE Verifiers (Gemini Flash)...\")\n",
    "        \n",
    "        # Load Memory\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL,\n",
    "            model_kwargs={'device': 'cuda'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        self.db = Chroma(persist_directory=DB_DIR, embedding_function=self.embeddings)\n",
    "        \n",
    "        # Load Brain\n",
    "        if not MY_GEMINI_KEY or \"...\" in MY_GEMINI_KEY:\n",
    "            raise ValueError(\"‚ùå You forgot to paste your Gemini API Key!\")\n",
    "\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            google_api_key=MY_GEMINI_KEY,\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Attach Verifier\n",
    "        self.verifier = MoEVerifier(self.llm)\n",
    "        \n",
    "        # Navigator Prompt\n",
    "        self.navigator_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are a Research Agent.\n",
    "        \n",
    "        GOAL: {goal}\n",
    "        \n",
    "        CURRENT INFORMATION FOUND:\n",
    "        {context}\n",
    "        \n",
    "        INSTRUCTIONS:\n",
    "        1. Read the text. \n",
    "        2. If you have the COMPLETE answer, output: ANSWER: [Your Answer]\n",
    "        3. If you need to search a related topic, output: HOP: [Topic Name]\n",
    "        \n",
    "        CONSTRAINT: Output ONLY the line starting with ANSWER or HOP.\n",
    "        \"\"\")\n",
    "        \n",
    "    def search(self, query):\n",
    "        results = self.db.similarity_search(query, k=1)\n",
    "        return results[0] if results else None\n",
    "\n",
    "    def solve(self, user_query, max_hops=3):\n",
    "        print(f\"\\nüöÄ THINKING PROCESS: '{user_query}'\")\n",
    "        current_query = user_query\n",
    "        visited_context = []\n",
    "        \n",
    "        for step in range(max_hops):\n",
    "            print(f\"   üë£ Step {step + 1}: Searching for '{current_query}'...\")\n",
    "            time.sleep(1) \n",
    "            \n",
    "            node = self.search(current_query)\n",
    "            if not node:\n",
    "                print(\"      ‚ö†Ô∏è Dead end in graph.\")\n",
    "                break\n",
    "                \n",
    "            content = node.page_content\n",
    "            source = node.metadata.get('title', 'Unknown')\n",
    "            neighbors = node.metadata.get('graph_neighbors', 'None')\n",
    "            \n",
    "            print(f\"      üìÑ Reading: {source}\")\n",
    "            \n",
    "            # Store context for the Verifier to check later\n",
    "            visited_context.append(f\"SOURCE: {source}\\nCONTENT: {content}\\nLINKS: {neighbors}\")\n",
    "            full_context = \"\\n\\n\".join(visited_context)\n",
    "            \n",
    "            # Ask LLM what to do\n",
    "            chain = self.navigator_prompt | self.llm | StrOutputParser()\n",
    "            try:\n",
    "                decision = chain.invoke({\"goal\": user_query, \"context\": full_context})\n",
    "                decision = decision.strip()\n",
    "                \n",
    "                match = re.search(r\"(ANSWER|HOP):\\s*(.*)\", decision, re.DOTALL)\n",
    "                if match:\n",
    "                    action = match.group(1)\n",
    "                    value = match.group(2).strip()\n",
    "                    \n",
    "                    if action == \"ANSWER\":\n",
    "                        # *** CRITICAL STEP: VERIFY BEFORE RETURNING ***\n",
    "                        is_valid, reason = self.verifier.verify(user_query, value, full_context)\n",
    "                        \n",
    "                        if is_valid:\n",
    "                            return value\n",
    "                        else:\n",
    "                            return f\"‚ùå I found an answer, but my MoE Council rejected it.\\nReason: {reason}\"\n",
    "                    \n",
    "                    elif action == \"HOP\":\n",
    "                        \n",
    "                        print(f\"      üîó Hopping to -> {value}\")\n",
    "                        current_query = value\n",
    "                else:\n",
    "                    if step == max_hops - 1: return decision\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return f\"‚ùå Error: {e}\"\n",
    "\n",
    "        return \"‚ùå I searched the graph but could not find a complete answer within the limit.\"\n",
    "\n",
    "# ==========================================\n",
    "# 3. THE CHAT LOOP (USER INTERFACE)\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    agent = GraphRAGAgent()\n",
    "    \n",
    "    # PUT YOUR QUESTION HERE\n",
    "    question = \"what is civil department?\"\n",
    "    \n",
    "    print(f\"Asking: {question}\")\n",
    "    response = agent.solve(question)\n",
    "    \n",
    "    print(\"FINAL ANSWER:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7390e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# üîπ import your LLM function\n",
    "# example:\n",
    "# from graphmind import run_graphmind\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# ‚úÖ allow frontend to talk to backend\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    query: str\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def chat(req: ChatRequest):\n",
    "    question = req.query\n",
    "\n",
    "    # üî• CALL YOUR EXISTING LLM CODE HERE\n",
    "    # answer = run_graphmind(question)\n",
    "\n",
    "    answer = \"Backend connected successfully\"  # temporary test\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
